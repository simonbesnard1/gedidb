stages:          # List of stages for jobs, and their order of execution
  - build
  - test
#  - deploy
#
#build-job:
#  stage: build
#  script:
#    # Install conda (if not already installed on the runner)
#    - wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
#    - bash miniconda.sh -b -p $HOME/miniconda
#    - source "$HOME/miniconda/etc/profile.d/conda.sh"
#    - conda activate
#    - conda config --set always_yes yes
#    - conda env create -f ci/requirements/doc.yml  # Use your doc.yml to create the environment
#    - conda activate gedidb-docs  # Activate the environment
#    # Build the documentation
#    - sphinx-build -b html doc/ doc/_build/html
#  artifacts:
#    paths:
#      - doc/_build/html  # Store the build results as artifacts
#    expire_in: 1 day     # Optional: Define how long to keep artifacts

setup-job:
  stage: build
  script:
    - python -m venv .venv
    - pip install -e .
    - pip install geodatasets
    - pip install flake8 # linter
    - pip install yaml # for testing local files
  artifacts:
    paths:
    - .venv
    expire_in: 1 day

test-gedi-granules:
  stage: test
  dependencies:
    - setup-job
  script:
    - echo "Running test_gedi_granules.py..."
    - source .venv/bin/activate
    - python -m unittest gedidb.tests.test_gedi_granules

test-granule-name:
  stage: test
  dependencies:
    - setup-job
  script:
    - echo "Running test_granule_name.py..."
    - source .venv/bin/activate
    - python -m unittest gedidb.tests.test_granule_name

test-nasa-cmr-api:
  stage: test
  dependencies:
    - setup-job
  script:
    - echo "Running test_nasa_cmr_api.py..."
    - source .venv/bin/activate
    - python -m unittest gedidb.tests.test_nasa_cmr_api

test-geospatial-tools:
  stage: test
  dependencies:
    - setup-job
  script:
    - echo "Running test_geospatial_tools.py..."
    - source .venv/bin/activate
    - python -m unittest gedidb.tests.test_geospatial_tools

test-local-tiledb:
  stage: test
  dependencies:
    - setup-job
  script:
    - echo "Running test_local_tiledb.py..."
    - source .venv/bin/activate
    - python -m unittest gedidb.tests.test_local_tiledb

test-data-config-yml:
  stage: test
  dependencies:
    - setup-job
  script:
    - echo "Running test_data_config_yml.py..."
    - source .venv/bin/activate
    - python -m unittest gedidb.tests.test_data_config_yml

#test-data-downloader:
#  stage: test
#  before_script:
#    - pip install -e . # might there be a better solution to this? increases the needed time for unittests a looot
#  script:
#    - echo "Running test_data_downloader.py..."
#    - python -m unittest gedidb.tests.test_data_downloader

lint-test-job:   # This job also runs in the test stage.
  stage: test    # It can run at the same time as unit-test-job (in parallel).
  before_script:
    - pip install flake8
  script:
    - echo "Linting code..."
    - python -m flake8

#deploy-job:      # This job runs in the deploy stage.
#  stage: deploy  # It only runs when *both* jobs in the test stage complete successfully.
#  environment: production
#  script:
#    - echo "Deploying application..."
#    - echo "Application successfully deployed."
#
#pages:  # this job must be called 'pages' to advise GitLab to upload content to GitLab Pages
#  stage: deploy
#  dependencies:
#    - build-job  # Ensure that the pages job depends on the build-job
#  script:
#    # Create the public directory
#    - rm -rf public
#    - mkdir public
#    - mkdir -p public/doc
#    - mkdir -p public/doc/_static/
#
#    # Copy over the docs
#    - cp doc/index.html public/
#    - cp -r doc/_build/html/* public/doc/
#
#    # Check if everything is working
#    - ls -al public
#    - ls -al public/doc
#  artifacts:
#    paths:
#      - public
#    expire_in: 30 days
#  only:
#    - documentation
#    - main
#    - staging
